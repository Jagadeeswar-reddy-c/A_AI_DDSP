{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c7fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a773de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung forecasts_ALL VAR.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung forecasts_ALL VAR.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung forecasts_ALL VAR.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung forecasts_VAR 02.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung forecasts_VAR 02.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung forecasts_VAR 02.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung historics_per BL.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Bevölkerung historics_per BL.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung historics_per BL.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2021.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2021.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2021.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2022.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2022.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2022.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2023.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_KHV_2023.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2023.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2021.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2021.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2021.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2022.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2022.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2022.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2023.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\DSB_RHV_2023.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2023.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Impressum.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Impressum.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Impressum.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Inhalt.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Inhalt.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Inhalt.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2021.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2021.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2021.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2022.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2022.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2022.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2023.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\KHV_2023.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2023.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Kreis.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Kreis.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Kreis.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Land.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Land.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Land.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Raw Data.csv\n",
      "Error processing C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Raw Data.csv: No columns to parse from file\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2021.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2021.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2021.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2022.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2022.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2022.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2023.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\RHV_2023.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2023.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Sheet1.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Sheet1.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Sheet1.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Statistische Ämter.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Statistische Ämter.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Statistische Ämter.csv\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Titelblatt.csv\n",
      "Error processing C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Titelblatt.csv: No columns to parse from file\n",
      "Processing file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Vorbemerkungen.csv\n",
      "Successfully preprocessed C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\\Vorbemerkungen.csv\n",
      "Saved cleaned data to: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Vorbemerkungen.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def preprocess_csv(file_path):\n",
    "    \"\"\"\n",
    "    Preprocesses a single CSV file by skipping initial empty rows,\n",
    "    combining two header rows, and removing empty columns.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the file, skipping the initial empty rows.\n",
    "        # It's observed that there are 4 empty rows, so we start reading from row 4 (0-indexed).\n",
    "        # We read the next two rows (index 4 and 5) as potential header rows.\n",
    "        df_header_part1 = pd.read_csv(file_path, header=None, skiprows=3, nrows=1)\n",
    "        df_header_part2 = pd.read_csv(file_path, header=None, skiprows=4, nrows=1)\n",
    "\n",
    "        # Read the actual data, skipping the initial empty rows and the two header rows\n",
    "        df_data = pd.read_csv(file_path, header=None, skiprows=5)\n",
    "\n",
    "        # Combine the two header parts\n",
    "        # For columns that are not years (e.g., 'Varianten der Bevölkerungsvorausberechnung', 'Geschlecht', 'Altersjahre'),\n",
    "        # the header is simply the value from df_header_part1.\n",
    "        # For year columns, combine both parts: e.g., '2022 - 31.12.2022'\n",
    "\n",
    "        # Initialize the new header list\n",
    "        new_header = []\n",
    "\n",
    "        # Iterate through the columns and combine headers\n",
    "        for col_idx in range(len(df_header_part1.columns)):\n",
    "            # Check if both parts of the header exist for this column\n",
    "            header_part1_val = str(df_header_part1.iloc[0, col_idx]).strip() if col_idx < len(df_header_part1.columns) else ''\n",
    "            header_part2_val = str(df_header_part2.iloc[0, col_idx]).strip() if col_idx < len(df_header_part2.columns) else ''\n",
    "\n",
    "            if header_part1_val and header_part2_val and header_part1_val != 'nan' and header_part2_val != 'nan':\n",
    "                # If the second part looks like a date, combine them\n",
    "                if any(char.isdigit() for char in header_part2_val) and '.' in header_part2_val:\n",
    "                    new_header.append(f\"{header_part1_val} - {header_part2_val}\")\n",
    "                else:\n",
    "                    new_header.append(header_part1_val) # Use the first part if second is not a date\n",
    "            elif header_part1_val and header_part1_val != 'nan':\n",
    "                new_header.append(header_part1_val)\n",
    "            elif header_part2_val and header_part2_val != 'nan':\n",
    "                new_header.append(header_part2_val)\n",
    "            else:\n",
    "                new_header.append(f\"Unnamed_Col_{col_idx}\") # Fallback for completely empty header parts\n",
    "\n",
    "\n",
    "        # Assign the new header to the data DataFrame\n",
    "        df_data.columns = new_header[:len(df_data.columns)] # Ensure header length matches data columns\n",
    "\n",
    "        # Remove columns that are entirely empty or contain only 'e'\n",
    "        # First, drop columns where all values are NaN\n",
    "        df_data.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "        # Then, drop columns where all values are 'e' (if they exist after the initial dropna)\n",
    "        # Convert all values to string to handle 'e' consistently\n",
    "        df_data = df_data.loc[:, ~df_data.astype(str).apply(lambda x: (x == 'e').all())]\n",
    "        \n",
    "        # Also remove columns where the header might be 'Unnamed_Col_X' and data is empty\n",
    "        # This covers cases where 'e' was removed and no proper header was found\n",
    "        cols_to_drop_by_name = [col for col in df_data.columns if 'Unnamed_Col_' in str(col) and df_data[col].isnull().all()]\n",
    "        df_data.drop(columns=cols_to_drop_by_name, inplace=True)\n",
    "\n",
    "\n",
    "        print(f\"Successfully preprocessed {file_path}\")\n",
    "        return df_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_multiple_csv_files(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in a given input directory and saves the\n",
    "    preprocessed files to an output directory.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): The path to the directory containing input CSV files.\n",
    "        output_directory (str): The path to the directory where preprocessed files will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "        print(f\"Created output directory: {output_directory}\")\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            preprocessed_df = preprocess_csv(file_path)\n",
    "\n",
    "            if preprocessed_df is not None:\n",
    "                output_file_path = os.path.join(output_directory, f\"cleaned_{filename}\")\n",
    "                preprocessed_df.to_csv(output_file_path, index=False)\n",
    "                print(f\"Saved cleaned data to: {output_file_path}\")\n",
    "\n",
    "# --- How to use the code ---\n",
    "# 1. Replace 'your_input_csv_directory' with the path to the folder containing your CSV files.\n",
    "# 2. Replace 'your_output_csv_directory' with the path to the folder where you want to save the cleaned files.\n",
    "#    If the output directory doesn't exist, the code will create it.\n",
    "\n",
    "# Example usage:\n",
    "input_dir = r\"C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\raw\" # e.g., 'data/raw_csvs'\n",
    "output_dir = r\"C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\" # e.g., 'data/processed_csvs'\n",
    "\n",
    "# # For demonstration, let's assume the user has the 'Bevölkerung forecasts_VAR 02.csv' in the current directory\n",
    "# # and wants to save the output in a folder named 'output_csvs' in the same directory.\n",
    "# input_dir = '.' # Current directory\n",
    "# output_dir = 'output_csvs'\n",
    "\n",
    "process_multiple_csv_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fc876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea0def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6c211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4608e30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create database 'hospital_data' if it doesn't exist...\n",
      "Database 'hospital_data' ensured to exist.\n",
      "Executing: -- Table for File 1: Population Projection Data\n",
      "-- This table stores population counts based on diff...\n",
      "OK\n",
      "Executing: -- Table for File 2: Hospital Bed Details\n",
      "-- This table serves as a lookup for detailed hospital dep...\n",
      "OK\n",
      "Executing: -- Table for File 3: Comprehensive Hospital Facilities Data\n",
      "-- This table stores comprehensive infor...\n",
      "OK\n",
      "Executing: -- Table for File 4: Summarized Hospital Facilities Data\n",
      "-- This table provides a summarized view of...\n",
      "OK\n",
      "All tables created successfully (or already existed).\n",
      "MySQL connection closed.\n",
      "\n",
      "Database and tables setup complete.\n",
      "Read 8899 lines from population projection file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung forecasts_ALL VAR.csv\n",
      "\n",
      "Inserting 8899 rows into PopulationProjections...\n",
      "8899 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 330 lines from population projection file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung forecasts_VAR 02.csv\n",
      "\n",
      "Inserting 330 rows into PopulationProjections...\n",
      "330 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 1028 lines from population projection file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_Bevölkerung historics_per BL.csv\n",
      "\n",
      "Inserting 1028 rows into PopulationProjections...\n",
      "1028 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 60 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2023.csv\n",
      "\n",
      "Inserting 60 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 60 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2022.csv\n",
      "\n",
      "Inserting 60 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 63 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_RHV_2021.csv\n",
      "\n",
      "Inserting 63 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 182 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2023.csv\n",
      "\n",
      "Inserting 182 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 182 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2022.csv\n",
      "\n",
      "Inserting 182 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 183 lines from hospital bed details file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_DSB_KHV_2021.csv\n",
      "\n",
      "Inserting 183 rows into HospitalBedDetails...\n",
      "Error inserting hospital bed details: 1062 (23000): Duplicate entry '' for key 'hospitalbeddetails.FieldName'\n",
      "MySQL connection closed.\n",
      "Read 2768 lines from comprehensive hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2021.csv\n",
      "\n",
      "Inserting 2768 rows into HospitalFacilitiesComprehensive...\n",
      "2768 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 2785 lines from comprehensive hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2022.csv\n",
      "\n",
      "Inserting 2785 rows into HospitalFacilitiesComprehensive...\n",
      "2785 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 2780 lines from comprehensive hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_KHV_2023.csv\n",
      "\n",
      "Inserting 2780 rows into HospitalFacilitiesComprehensive...\n",
      "2780 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 1077 lines from summarized hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2023.csv\n",
      "\n",
      "Inserting 1077 rows into HospitalFacilitiesSummarized...\n",
      "1077 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 1086 lines from summarized hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2022.csv\n",
      "\n",
      "Inserting 1086 rows into HospitalFacilitiesSummarized...\n",
      "1086 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "Read 1091 lines from summarized hospital facilities file: C:\\Users\\user\\Desktop\\A_AI_DDSP\\data\\processed\\Processed\\cleaned_RHV_2021.csv\n",
      "\n",
      "Inserting 1091 rows into HospitalFacilitiesSummarized...\n",
      "1091 rows inserted successfully.\n",
      "MySQL connection closed.\n",
      "\n",
      "CSV data import process completed.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# --- MySQL Database Configuration ---\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root', # User with CREATE DATABASE privilege (e.g., 'root')\n",
    "    'password': 'Pavani@3534',\n",
    "    'database': 'hospital_data' # Name of the database to create/use\n",
    "}\n",
    "\n",
    "# --- CSV File Paths ---\n",
    "# NOTE: Updated with the exact paths provided in the error output\n",
    "CSV_FILE_PATHS = {\n",
    "    'population': [\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_Bevölkerung forecasts_ALL VAR.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_Bevölkerung forecasts_VAR 02.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_Bevölkerung historics_per BL.csv'\n",
    "    ],\n",
    "    'bed_details': [\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_RHV_2023.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_RHV_2022.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_RHV_2021.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_KHV_2023.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_KHV_2022.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_DSB_KHV_2021.csv'\n",
    "    ],\n",
    "    'comprehensive': [\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_KHV_2021.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_KHV_2022.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_KHV_2023.csv'\n",
    "    ],\n",
    "    'summarized': [\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_RHV_2023.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_RHV_2022.csv',\n",
    "        'C:\\\\Users\\\\user\\\\Desktop\\\\A_AI_DDSP\\\\data\\\\processed\\\\Processed\\\\cleaned_RHV_2021.csv'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- SQL CREATE TABLE Statements (Updated to FLOAT for population data) ---\n",
    "SQL_CREATE_TABLES = \"\"\"\n",
    "-- Table for File 1: Population Projection Data\n",
    "-- This table stores population counts based on different projection variants, gender, age, and specific dates.\n",
    "CREATE TABLE IF NOT EXISTS PopulationProjections (\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT, -- Unique identifier for each row\n",
    "    ProjectionVariant VARCHAR(255),    -- Description of the population projection variant\n",
    "    Gender VARCHAR(50),                -- Gender (e.g., 'männlich', 'weiblich', 'Insgesamt')\n",
    "    AgeGroup VARCHAR(50),              -- Age or age group (e.g., 'unter 1 Jahr', '1-Jährige', '90+', 'Insgesamt')\n",
    "    Population_2022_12_31 FLOAT,         -- Population count for 2022-12-31\n",
    "    Population_2023_12_31 FLOAT,         -- Population count for 2023-12-31\n",
    "    Population_2024_12_31 FLOAT,         -- Population count for 2024-12-31\n",
    "    Population_2025_12_31 FLOAT,         -- Population count for 2025-12-31\n",
    "    Population_2026_12_31 FLOAT,         -- Population count for 2026-12-31\n",
    "    Population_2027_12_31 FLOAT,         -- Population count for 2027-12-31\n",
    "    Population_2028_12_31 FLOAT,         -- Population count for 2028-12-31\n",
    "    Population_2029_12_31 FLOAT,         -- Population count for 2029-12-31\n",
    "    Population_2030_12_31 FLOAT,         -- Population count for 2030-12-31\n",
    "    Population_2031_12_31 FLOAT,         -- Population count for 2031-12-31\n",
    "    Population_2032_12_31 FLOAT,         -- Population count for 2032-12-31\n",
    "    Population_2033_12_31 FLOAT,         -- Population count for 2033-12-31\n",
    "    Population_2034_12_31 FLOAT,         -- Population count for 2034-12-31\n",
    "    Population_2035_12_31 FLOAT,         -- Population count for 2035-12-31\n",
    "    Population_2036_12_31 FLOAT,         -- Population count for 2036-12-31\n",
    "    Population_2037_12_31 FLOAT,         -- Population count for 2037-12-31\n",
    "    Population_2038_12_31 FLOAT,         -- Population count for 2038-12-31\n",
    "    Population_2039_12_31 FLOAT,         -- Population count for 2039-12-31\n",
    "    Population_2040_12_31 FLOAT,         -- Population count for 2040-12-31\n",
    "    Population_2041_12_31 FLOAT,         -- Population count for 2041-12-31\n",
    "    Population_2042_12_31 FLOAT,         -- Population count for 2042-12-31\n",
    "    Population_2043_12_31 FLOAT,         -- Population count for 2043-12-31\n",
    "    Population_2044_12_31 FLOAT,         -- Population count for 2044-12-31\n",
    "    Population_2045_12_31 FLOAT,         -- Population count for 2045-12-31\n",
    "    Population_2046_12_31 FLOAT,         -- Population count for 2046-12-31\n",
    "    Population_2047_12_31 FLOAT,         -- Population count for 2047-12-31\n",
    "    Population_2048_12_31 FLOAT,         -- Population count for 2048-12-31\n",
    "    Population_2049_12_31 FLOAT,         -- Population count for 2049-12-31\n",
    "    Population_2050_12_31 FLOAT,         -- Population count for 2050-12-31\n",
    "    Population_2051_12_31 FLOAT,         -- Population count for 2051-12-31\n",
    "    Population_2052_12_31 FLOAT,         -- Population count for 2052-12-31\n",
    "    Population_2053_12_31 FLOAT,         -- Population count for 2053-12-31\n",
    "    Population_2054_12_31 FLOAT,         -- Population count for 2054-12-31\n",
    "    Population_2055_12_31 FLOAT,         -- Population count for 2055-12-31\n",
    "    Population_2056_12_31 FLOAT,         -- Population count for 2056-12-31\n",
    "    Population_2057_12_31 FLOAT,         -- Population count for 2057-12-31\n",
    "    Population_2058_12_31 FLOAT,         -- Population count for 2058-12-31\n",
    "    Population_2059_12_31 FLOAT,         -- Population count for 2059-12-31\n",
    "    Population_2060_12_31 FLOAT,         -- Population count for 2060-12-31\n",
    "    Population_2061_12_31 FLOAT,         -- Population count for 2061-12-31\n",
    "    Population_2062_12_31 FLOAT,         -- Population count for 2062-12-31\n",
    "    Population_2063_12_31 FLOAT,         -- Population count for 2063-12-31\n",
    "    Population_2064_12_31 FLOAT,         -- Population count for 2064-12-31\n",
    "    Population_2065_12_31 FLOAT,         -- Population count for 2065-12-31\n",
    "    Population_2066_12_31 FLOAT,         -- Population count for 2066-12-31\n",
    "    Population_2067_12_31 FLOAT,         -- Population count for 2067-12-31\n",
    "    Population_2068_12_31 FLOAT,         -- Population count for 2068-12-31\n",
    "    Population_2069_12_31 FLOAT,         -- Population count for 2069-12-31\n",
    "    Population_2070_12_31 FLOAT          -- Population count for 2070-12-31\n",
    ");\n",
    "\n",
    "-- Table for File 2: Hospital Bed Details\n",
    "-- This table serves as a lookup for detailed hospital department codes and their descriptions.\n",
    "CREATE TABLE IF NOT EXISTS HospitalBedDetails (\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT, -- Unique identifier for each row\n",
    "    FieldName VARCHAR(255) UNIQUE,     -- The field name or code (e.g., 'Kreis', '0100', 'INSG')\n",
    "    Content TEXT                       -- The description or content associated with the field name\n",
    ");\n",
    "\n",
    "-- Table for File 3: Comprehensive Hospital Facilities Data\n",
    "-- This table stores comprehensive information about hospital facilities, including location, contact,\n",
    "-- ownership, type, emergency care provisions, and detailed bed counts by department.\n",
    "CREATE TABLE IF NOT EXISTS HospitalFacilitiesComprehensive (\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT, -- Unique identifier for each hospital entry\n",
    "    Land VARCHAR(100),                 -- State or federal state in Germany\n",
    "    Kreis VARCHAR(100),                -- District/County (with first digit potentially indicating administrative region)\n",
    "    Gemeinde VARCHAR(100),             -- Municipality\n",
    "    HospitalName VARCHAR(255),         -- Name of the hospital\n",
    "    SiteName VARCHAR(255),             -- Name of the hospital site/location\n",
    "    SiteStreet VARCHAR(255),           -- Street name of the hospital site\n",
    "    SiteHouseNumber VARCHAR(50),       -- House number of the hospital site\n",
    "    SitePostalCode VARCHAR(20),        -- Postal code of the hospital site\n",
    "    SiteCity VARCHAR(100),             -- City/Town of the hospital site\n",
    "    PhoneNumber VARCHAR(50),           -- Full telephone number (area code and number)\n",
    "    EmailAddress VARCHAR(255),         -- Email address of the hospital\n",
    "    InternetAddress VARCHAR(255),      -- Website address of the hospital\n",
    "    OwnerType INT,                     -- Type of ownership (1=public, 2=non-profit, 3=private)\n",
    "    OwnerName VARCHAR(255),            -- Name of the owner/operator\n",
    "    InstitutionType INT,               -- Type of institution (e.g., 1=university clinic, 2=planned hospital, etc.)\n",
    "    GeneralEmergencyCare INT,          -- Level of general stationary emergency care (0=none, 1=basic, 2=extended, 3=comprehensive)\n",
    "    -- Note: Assuming these are distinct columns for special emergency care modules.\n",
    "    -- If they are meant to be a single column with multiple values, consider a separate linking table or JSON/TEXT field.\n",
    "    SpecialEmergencyCare_Modul1 VARCHAR(255), -- Special stationary emergency care module 1\n",
    "    SpecialEmergencyCare_Modul2 VARCHAR(255), -- Special stationary emergency care module 2\n",
    "    SpecialEmergencyCare_Modul3 VARCHAR(255), -- Special stationary emergency care module 3\n",
    "    SpecialEmergencyCare_Modul4 VARCHAR(255), -- Special stationary emergency care module 4\n",
    "    SpecialEmergencyCare_Modul5 VARCHAR(255), -- Special stationary emergency care module 5\n",
    "    TotalBeds INT,                     -- Total number of established beds (INSG)\n",
    "\n",
    "    -- Detailed Bed Counts by Department (based on File 2's descriptions)\n",
    "    Beds_100_118 INT,                  -- Internal Medicine\n",
    "    Beds_102 INT,                      -- Internal Medicine/Geriatrics\n",
    "    Beds_103 INT,                      -- Internal Medicine/Cardiology\n",
    "    Beds_104 INT,                      -- Internal Medicine/Nephrology\n",
    "    Beds_105 INT,                      -- Internal Medicine/Hematology and Internal Oncology\n",
    "    Beds_106 INT,                      -- Internal Medicine/Endocrinology\n",
    "    Beds_107 INT,                      -- Internal Medicine/Gastroenterology\n",
    "    Beds_108 INT,                      -- Internal Medicine/Pneumology\n",
    "    Beds_109 INT,                      -- Internal Medicine/Rheumatology\n",
    "    Beds_114 INT,                      -- Internal Medicine/Pulmonary and Bronchial Medicine\n",
    "    Beds_152 INT,                      -- Internal Medicine/Infectious Diseases\n",
    "    Beds_153 INT,                      -- Internal Medicine/Diabetes\n",
    "    Beds_154 INT,                      -- Internal Medicine/Naturopathy\n",
    "    Beds_156 INT,                      -- Internal Medicine/Stroke units\n",
    "    Beds_200 INT,                      -- Geriatrics\n",
    "    Beds_300 INT,                      -- Cardiology\n",
    "    Beds_400 INT,                      -- Nephrology\n",
    "    Beds_500 INT,                      -- Hematology and Internal Oncology\n",
    "    Beds_510 INT,                      -- Hematology and Internal Oncology/Pediatrics\n",
    "    Beds_533 INT,                      -- Hematology and Internal Oncology/Radiotherapy\n",
    "    Beds_600 INT,                      -- Endocrinology\n",
    "    Beds_700 INT,                      -- Gastroenterology\n",
    "    Beds_800 INT,                      -- Pneumology\n",
    "    Beds_900 INT,                      -- Rheumatology\n",
    "    Beds_1000_34 INT,                  -- Pediatrics\n",
    "    Beds_1004 INT,                     -- Pediatrics/Nephrology\n",
    "    Beds_1005 INT,                     -- Pediatrics/Hematology and Internal Oncology\n",
    "    Beds_1007 INT,                     -- Pediatrics/Gastroenterology\n",
    "    Beds_1009 INT,                     -- Pediatrics/Rheumatology\n",
    "    Beds_1011 INT,                     -- Pediatrics/Pediatric Cardiology\n",
    "    Beds_1012 INT,                     -- Pediatrics/Neonatology\n",
    "    Beds_1014 INT,                     -- Pediatrics/Pulmonary and Bronchial Medicine\n",
    "    Beds_1028 INT,                     -- Pediatrics/Pediatric Neurology\n",
    "    Beds_1051 INT,                     -- Long-term care for children\n",
    "    Beds_1100 INT,                     -- Pediatric Cardiology\n",
    "    Beds_1136 INT,                     -- Pediatric Cardiology/Intensive Care Medicine\n",
    "    Beds_1200 INT,                     -- Neonatology\n",
    "    Beds_1300 INT,                     -- Pediatric Surgery\n",
    "    Beds_1400 INT,                     -- Pulmonary and Bronchial Medicine\n",
    "    Beds_1500 INT,                     -- General Surgery\n",
    "    Beds_1513 INT,                     -- General Surgery/Pediatric Surgery\n",
    "    Beds_1516 INT,                     -- General Surgery/Trauma Surgery\n",
    "    Beds_1518 INT,                     -- General Surgery/Vascular Surgery\n",
    "    Beds_1519 INT,                     -- General Surgery/Plastic Surgery\n",
    "    Beds_1520 INT,                     -- General Surgery/Thoracic Surgery\n",
    "    Beds_1523 INT,                     -- Surgery/Orthopedics\n",
    "    Beds_1536 INT,                     -- General Surgery/Intensive Care Medicine\n",
    "    Beds_1550 INT,                     -- General Surgery/Abdominal and Vascular Surgery\n",
    "    Beds_1551 INT,                     -- General Surgery/Hand Surgery\n",
    "    Beds_1600_59 INT,                  -- Trauma Surgery\n",
    "    Beds_1700_35 INT,                  -- Neurosurgery\n",
    "    Beds_1800_28 INT,                  -- Vascular Surgery\n",
    "    Beds_1900 INT,                     -- Plastic Surgery\n",
    "    Beds_2000 INT,                     -- Thoracic Surgery\n",
    "    Beds_2021 INT,                     -- Thoracic Surgery/Cardiac Surgery\n",
    "    Beds_2036 INT,                     -- Thoracic Surgery/Intensive Care Medicine\n",
    "    Beds_2050 INT,                     -- Thoracic Surgery/Cardiac Surgery Intensive Care Medicine\n",
    "    Beds_2100 INT,                     -- Cardiac Surgery\n",
    "    Beds_2118 INT,                     -- Cardiac Surgery/Vascular Surgery\n",
    "    Beds_2120 INT,                     -- Cardiac Surgery/Thoracic Surgery\n",
    "    Beds_2136 INT,                     -- Cardiac Surgery/Intensive Care Medicine\n",
    "    Beds_2150 INT,                     -- Cardiac Surgery/Thoracic Surgery Intensive Care Medicine\n",
    "    Beds_2200_39 INT,                  -- Urology\n",
    "    Beds_2300 INT,                     -- Orthopedics\n",
    "    Beds_2309 INT,                     -- Orthopedics/Rheumatology\n",
    "    Beds_2315 INT,                     -- Orthopedics/Surgery\n",
    "    Beds_2316 INT,                     -- Orthopedics and Trauma Surgery\n",
    "    Beds_2400_41 INT,                  -- Gynecology and Obstetrics\n",
    "    Beds_2402 INT,                     -- Gynecology/Geriatrics\n",
    "    Beds_2405 INT,                     -- Gynecology/Hematology and Internal Oncology\n",
    "    Beds_2406 INT,                     -- Gynecology/Endocrinology\n",
    "    Beds_2425 INT,                     -- Gynecology\n",
    "    Beds_2500 INT,                     -- Obstetrics\n",
    "    Beds_2600 INT,                     -- Otorhinolaryngology (ENT)\n",
    "    Beds_2700 INT,                     -- Ophthalmology\n",
    "    Beds_2800_46 INT,                  -- Neurology\n",
    "    Beds_2810 INT,                     -- Neurology/Pediatrics\n",
    "    Beds_2851 INT,                     -- Neurology/Gerontology\n",
    "    Beds_2852 INT,                     -- Neurology/Neurological Early Rehabilitation\n",
    "    Beds_2856 INT,                     -- Neurology/Stroke Patients\n",
    "    Beds_2900 INT,                     -- General Psychiatry\n",
    "    Beds_2928 INT,                     -- General Psychiatry/Neurology\n",
    "    Beds_2930 INT,                     -- General Psychiatry/Child and Adolescent Psychiatry\n",
    "    Beds_2931 INT,                     -- General Psychiatry/Psychosomatics/Psychotherapy\n",
    "    Beds_2950 INT,                     -- General Psychiatry/Addiction Treatment\n",
    "    Beds_2951 INT,                     -- General Psychiatry/Gerontopsychiatry\n",
    "    Beds_2952 INT,                     -- General Psychiatry/Forensic Treatment\n",
    "    Beds_2953 INT,                     -- General Psychiatry/Addiction Treatment, Day Clinic\n",
    "    Beds_2954 INT,                     -- General Psychiatry/Addiction Treatment, Night Clinic\n",
    "    Beds_2955 INT,                     -- General Psychiatry/Gerontopsychiatry, Day Clinic\n",
    "    Beds_2956 INT,                     -- General Psychiatry/Gerontopsychiatry, Night Clinic\n",
    "    Beds_2960 INT,                     -- General Psychiatry/Day Clinic\n",
    "    Beds_2961 INT,                     -- General Psychiatry/Night Clinic\n",
    "    Beds_3000 INT,                     -- Child and Adolescent Psychiatry\n",
    "    Beds_3060 INT,                     -- Child and Adolescent Psychiatry/Day Clinic\n",
    "    Beds_3061 INT,                     -- Child and Adolescent Psychiatry/Night Clinic\n",
    "    Beds_3100 INT,                     -- Psychosomatics/Psychotherapy\n",
    "    Beds_3110 INT,                     -- Psychosomatics/Psychotherapy/Child and Adolescent Psychosomatics\n",
    "    Beds_3160 INT,                     -- Psychosomatics/Psychotherapy/Day Clinic\n",
    "    Beds_3161 INT,                     -- Psychosomatics/Psychotherapy/Night Clinic\n",
    "    Beds_3200 INT,                     -- Nuclear Medicine\n",
    "    Beds_3233 INT,                     -- Nuclear Medicine/Radiotherapy\n",
    "    Beds_3300 INT,                     -- Radiotherapy\n",
    "    Beds_3305 INT,                     -- Radiotherapy/Hematology and Internal Oncology\n",
    "    Beds_3350 INT,                     -- Radiotherapy/Radiology\n",
    "    Beds_3400 INT,                     -- Dermatology\n",
    "    Beds_3460 INT,                     -- Dermatology/Day Clinic\n",
    "    Beds_3500 INT,                     -- Dentistry and Oral and Maxillofacial Surgery\n",
    "    Beds_3600 INT,                     -- Intensive Care Medicine\n",
    "    Beds_3601 INT,                     -- Intensive Care Medicine/Internal Medicine\n",
    "    Beds_3603 INT,                     -- Intensive Care Medicine/Cardiology\n",
    "    Beds_3610 INT,                     -- Intensive Care Medicine/Pediatrics\n",
    "    B3617 INT,                     -- Intensive Care Medicine/Neurosurgery\n",
    "    Beds_3618 INT,                     -- Intensive Care Medicine/Surgery\n",
    "    Beds_3621 INT,                     -- Intensive Care Medicine/Cardiac Surgery\n",
    "    Beds_3622 INT,                     -- Intensive Care Medicine/Urology\n",
    "    Beds_3624 INT,                     -- Intensive Care Medicine/Gynecology and Obstetrics\n",
    "    Beds_3626 INT,                     -- Intensive Care Medicine/Otorhinolaryngology (ENT)\n",
    "    Beds_3628 INT,                     -- Intensive Care Medicine/Neurology\n",
    "    Beds_3650 INT,                     -- Operative Intensive Care Medicine/Surgery\n",
    "    Beds_3651 INT,                     -- Intensive Care Medicine/Thoracic-Cardiac Surgery\n",
    "    Beds_3652 INT,                     -- Intensive Care Medicine/Heart-Thoracic Surgery\n",
    "    Beds_3700 INT,                     -- Other Special Department\n",
    "    Beds_3750 INT,                     -- Angiology\n",
    "    Beds_3751 INT,                     -- Radiology\n",
    "    Beds_3752 INT,                     -- Palliative Medicine\n",
    "    Beds_3753 INT,                     -- Pain Therapy\n",
    "    Beds_3754 INT,                     -- Curative Therapy Department\n",
    "    Beds_3755 INT,                     -- Spinal Surgery\n",
    "    Beds_3756 INT,                     -- Addiction Medicine\n",
    "    Beds_3757 INT,                     -- Visceral Surgery\n",
    "    Beds_3758 INT                      -- Weaning Unit\n",
    ");\n",
    "\n",
    "-- Table for File 4: Summarized Hospital Facilities Data\n",
    "-- This table provides a summarized view of hospital facilities with broader bed count categories.\n",
    "CREATE TABLE IF NOT EXISTS HospitalFacilitiesSummarized (\n",
    "    id INT PRIMARY KEY AUTO_INCREMENT, -- Unique identifier for each hospital entry\n",
    "    Land VARCHAR(100),                 -- State or federal state in Germany\n",
    "    Kreis VARCHAR(100),                -- District/County\n",
    "    Gemeinde VARCHAR(100),             -- Municipality\n",
    "    HospitalName VARCHAR(255),         -- Name of the hospital\n",
    "    Street VARCHAR(255),               -- Street name\n",
    "    HouseNumber VARCHAR(50),           -- House number\n",
    "    PostalCode VARCHAR(20),            -- Postal code\n",
    "    City VARCHAR(100),                 -- City/Town\n",
    "    PhoneNumber VARCHAR(50),           -- Full telephone number (area code and number)\n",
    "    EmailAddress VARCHAR(255),         -- Email address of the hospital\n",
    "    InternetAddress VARCHAR(255),      -- Website address of the hospital\n",
    "    OwnerType INT,                     -- Type of ownership (1=public, 2=non-profit, 3=private)\n",
    "    OwnerName VARCHAR(255),            -- Name of the owner/operator\n",
    "    InstitutionType INT,               -- Type of institution (e.g., 1=university clinic, 2=planned hospital, etc.)\n",
    "    TotalBeds INT,                     -- Total number of established beds (INSG)\n",
    "\n",
    "    -- Summarized Bed Counts by Major Department\n",
    "    Beds_0 INT,                        -- Unknown/Unspecified beds (if this represents a general category)\n",
    "    Beds_100 INT,                      -- Internal Medicine (main category)\n",
    "    Beds_200 INT,                      -- Geriatrics (main category)\n",
    "    Beds_300 INT,                      -- Cardiology (main category)\n",
    "    Beds_400 INT,                      -- Nephrology (main category)\n",
    "    Beds_500 INT,                      -- Hematology and Internal Oncology (main category)\n",
    "    Beds_600 INT,                      -- Endocrinology (main category)\n",
    "    Beds_700 INT,                      -- Gastroenterology (main category)\n",
    "    Beds_800 INT,                      -- Pneumology (main category)\n",
    "    Beds_900 INT,                      -- Rheumatology (main category)\n",
    "    Beds_1000 INT,                     -- Pediatrics (main category)\n",
    "    Beds_1400 INT,                     -- Pulmonary and Bronchial Medicine (main category)\n",
    "    Beds_1500 INT,                     -- General Surgery (main category)\n",
    "    Beds_1600 INT,                     -- Trauma Surgery (main category)\n",
    "    Beds_1800 INT,                     -- Vascular Surgery (main category)\n",
    "    Beds_2100 INT,                     -- Cardiac Surgery (main category)\n",
    "    Beds_2200 INT,                     -- Urology (main category)\n",
    "    Beds_2300 INT,                     -- Orthopedics (main category)\n",
    "    Beds_2400 INT,                     -- Gynecology and Obstetrics (main category)\n",
    "    Beds_2600 INT,                     -- Otorhinolaryngology (ENT) (main category)\n",
    "    Beds_2700 INT,                     -- Ophthalmology (main category)\n",
    "    Beds_2800 INT,                     -- Neurology (main category)\n",
    "    Beds_2900 INT,                     -- General Psychiatry (main category)\n",
    "    Beds_3000 INT,                     -- Child and Adolescent Psychiatry (main category)\n",
    "    Beds_3100 INT,                     -- Psychosomatics/Psychotherapy (main category)\n",
    "    Beds_3400 INT,                     -- Dermatology (main category)\n",
    "    Beds_3700 INT,                     -- Other Special Department (main category)\n",
    "    Beds_8200 INT,                     -- Unknown/Other department category\n",
    "    Beds_8500_86 INT,                  -- Unknown/Other department category (range seems odd, consider renaming if possible)\n",
    "    Beds_8600 INT,                     -- Unknown/Other department category\n",
    "    Beds_8800 INT,                     -- Unknown/Other department category\n",
    "    Beds_9000 INT                      -- Unknown/Other department category\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# --- Helper Functions for Connection and Table Creation ---\n",
    "\n",
    "def create_database_if_not_exists(config):\n",
    "    \"\"\"\n",
    "    Connects to MySQL (without specifying a database) and creates the specified database if it doesn't exist.\n",
    "    \"\"\"\n",
    "    db_name = config['database']\n",
    "    temp_config = {k: v for k, v in config.items() if k != 'database'} # Connect without specific database\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**temp_config)\n",
    "        cursor = connection.cursor()\n",
    "        print(f\"Attempting to create database '{db_name}' if it doesn't exist...\")\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        connection.commit()\n",
    "        print(f\"Database '{db_name}' ensured to exist.\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"Error creating database '{db_name}': {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "def create_tables(config, sql_statements):\n",
    "    \"\"\"\n",
    "    Connects to the specified database and executes the CREATE TABLE statements.\n",
    "    Each table creation uses IF NOT EXISTS to prevent errors if the table already exists.\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config) # Connect to the specific database\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Split SQL statements by semicolon and execute each\n",
    "        statements = [s.strip() for s in sql_statements.split(';') if s.strip()]\n",
    "\n",
    "        for statement in statements:\n",
    "            try:\n",
    "                # Add IF NOT EXISTS to each CREATE TABLE statement if it's not already there\n",
    "                if statement.upper().startswith(\"CREATE TABLE\") and \"IF NOT EXISTS\" not in statement.upper():\n",
    "                    statement = statement.replace(\"CREATE TABLE\", \"CREATE TABLE IF NOT EXISTS\", 1)\n",
    "\n",
    "                print(f\"Executing: {statement[:100]}...\") # Print first 100 chars\n",
    "                cursor.execute(statement)\n",
    "                print(\"OK\")\n",
    "            except Error as err:\n",
    "                # With IF NOT EXISTS, this error check is less critical for table existence,\n",
    "                # but good for other potential DDL errors.\n",
    "                print(f\"Error executing statement: {err}\")\n",
    "                connection.rollback() # Rollback on error\n",
    "                return False # Indicate failure\n",
    "        connection.commit() # Commit all changes if no errors\n",
    "        print(\"All tables created successfully (or already existed).\")\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MySQL or during table creation: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "# --- CSV Reading and Parsing Functions ---\n",
    "\n",
    "def parse_population_line(parts):\n",
    "    \"\"\"\n",
    "    Parses a single line of population data (already split) and returns a dictionary\n",
    "    mapping column names to their values.\n",
    "    \"\"\"\n",
    "    # Extract fixed fields\n",
    "    # parts[0] is 'BEV-VARIANTE-01' or empty\n",
    "    # parts[1] is empty\n",
    "    # parts[2] is 'männlich'\n",
    "    # parts[3] is 'unter 1 Jahr'\n",
    "    \n",
    "    projection_variant = parts[0].strip() if parts[0].strip() else None\n",
    "    gender = parts[2].strip()\n",
    "    age_group = parts[3].strip()\n",
    "\n",
    "    # Extract population numbers, skipping the 'e' indicators\n",
    "    population_values = []\n",
    "    # Data starts from index 4, and every second element is 'e'\n",
    "    for i in range(4, len(parts), 2):\n",
    "        try:\n",
    "            # Replace comma with dot for float conversion\n",
    "            val = parts[i].strip().replace(',', '.')\n",
    "            population_values.append(float(val))\n",
    "        except ValueError:\n",
    "            population_values.append(None) # Handle cases where conversion fails\n",
    "\n",
    "    data = {\n",
    "        'ProjectionVariant': projection_variant,\n",
    "        'Gender': gender,\n",
    "        'AgeGroup': age_group,\n",
    "    }\n",
    "\n",
    "    # Dynamically assign population values to their respective year columns\n",
    "    # SQL columns are in the format Population_YYYY_12_31\n",
    "    current_year = 2022\n",
    "    for value in population_values:\n",
    "        col_name = f'Population_{current_year}_12_31'\n",
    "        data[col_name] = value\n",
    "        current_year += 1\n",
    "        if current_year > 2070: # Prevent going beyond the last year defined in the SQL table\n",
    "            break\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_population_data_from_csv(file_path):\n",
    "    \"\"\"Reads population projection data from a CSV file.\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Skip the header row.\n",
    "            # The original header is: \"Varianten der Bevölkerungsvorausberechnung\",,\"Geschlecht\",\"Altersjahre\",2022 - 31.12.2022,,2023 - 31.12.2023,,...\n",
    "            # The first data row is: \"BEV-VARIANTE-01\",,\"männlich\",\"unter 1 Jahr\",385.8,e,...\n",
    "            header_row = next(reader) \n",
    "            \n",
    "            for row in reader:\n",
    "                if row and len(row) > 3: # Ensure row is not empty and has enough columns\n",
    "                    parsed_row = parse_population_line(row)\n",
    "                    data.append(parsed_row)\n",
    "        print(f\"Read {len(data)} lines from population projection file: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading population projection file {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def insert_population_data(config, data_rows):\n",
    "    \"\"\"\n",
    "    Inserts parsed population data into the PopulationProjections table.\n",
    "    \"\"\"\n",
    "    if not data_rows:\n",
    "        print(\"No population data to insert.\")\n",
    "        return\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        columns = [\n",
    "            'ProjectionVariant', 'Gender', 'AgeGroup',\n",
    "            'Population_2022_12_31', 'Population_2023_12_31', 'Population_2024_12_31',\n",
    "            'Population_2025_12_31', 'Population_2026_12_31', 'Population_2027_12_31',\n",
    "            'Population_2028_12_31', 'Population_2029_12_31', 'Population_2030_12_31',\n",
    "            'Population_2031_12_31', 'Population_2032_12_31', 'Population_2033_12_31',\n",
    "            'Population_2034_12_31', 'Population_2035_12_31', 'Population_2036_12_31',\n",
    "            'Population_2037_12_31', 'Population_2038_12_31', 'Population_2039_12_31',\n",
    "            'Population_2040_12_31', 'Population_2041_12_31', 'Population_2042_12_31',\n",
    "            'Population_2043_12_31', 'Population_2044_12_31', 'Population_2045_12_31',\n",
    "            'Population_2046_12_31', 'Population_2047_12_31', 'Population_2048_12_31',\n",
    "            'Population_2049_12_31', 'Population_2050_12_31', 'Population_2051_12_31',\n",
    "            'Population_2052_12_31', 'Population_2053_12_31', 'Population_2054_12_31',\n",
    "            'Population_2055_12_31', 'Population_2056_12_31', 'Population_2057_12_31',\n",
    "            'Population_2058_12_31', 'Population_2059_12_31', 'Population_2060_12_31',\n",
    "            'Population_2061_12_31', 'Population_2062_12_31', 'Population_2063_12_31',\n",
    "            'Population_2064_12_31', 'Population_2065_12_31', 'Population_2066_12_31',\n",
    "            'Population_2067_12_31', 'Population_2068_12_31', 'Population_2069_12_31',\n",
    "            'Population_2070_12_31'\n",
    "        ]\n",
    "        \n",
    "        values_to_insert = []\n",
    "        for row_data in data_rows:\n",
    "            row_values = tuple(row_data.get(col, None) for col in columns)\n",
    "            values_to_insert.append(row_values)\n",
    "\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        insert_statement = f\"INSERT INTO PopulationProjections ({', '.join(columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "        print(f\"\\nInserting {len(values_to_insert)} rows into PopulationProjections...\")\n",
    "        cursor.executemany(insert_statement, values_to_insert)\n",
    "        connection.commit()\n",
    "        print(f\"{cursor.rowcount} rows inserted successfully.\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting population data: {e}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "def read_hospital_bed_details_from_csv(file_path):\n",
    "    \"\"\"Reads hospital bed details from a CSV file.\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Skip the header row.\n",
    "            # The original header is: \"Feldbezeichnung\",\"Inhalt\"\n",
    "            next(reader) \n",
    "            for row in reader:\n",
    "                if len(row) >= 2:\n",
    "                    data.append({\n",
    "                        'FieldName': row[0].strip(),\n",
    "                        'Content': row[1].strip()\n",
    "                    })\n",
    "        print(f\"Read {len(data)} lines from hospital bed details file: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading hospital bed details file {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def insert_hospital_bed_details_data(config, data_rows):\n",
    "    \"\"\"\n",
    "    Inserts data into the HospitalBedDetails table.\n",
    "    \"\"\"\n",
    "    if not data_rows:\n",
    "        print(\"No hospital bed details to insert.\")\n",
    "        return\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        insert_statement = \"INSERT INTO HospitalBedDetails (FieldName, Content) VALUES (%s, %s)\"\n",
    "        values_to_insert = [(row['FieldName'], row['Content']) for row in data_rows]\n",
    "\n",
    "        print(f\"\\nInserting {len(values_to_insert)} rows into HospitalBedDetails...\")\n",
    "        cursor.executemany(insert_statement, values_to_insert)\n",
    "        connection.commit()\n",
    "        print(f\"{cursor.rowcount} rows inserted successfully.\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting hospital bed details: {e}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "def read_hospital_comprehensive_data_from_csv(file_path):\n",
    "    \"\"\"Reads comprehensive hospital facilities data from a CSV file.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Mapping CSV column names to SQL column names\n",
    "    # This is for non-bed related columns\n",
    "    column_mapping = {\n",
    "        \"Land\": \"Land\", \"Kreis\": \"Kreis\", \"Gemeinde\": \"Gemeinde\", \"Adresse_Name\": \"HospitalName\",\n",
    "        \"Adresse_Name_Standort\": \"SiteName\", \"Adresse_Strasse_Standort\": \"SiteStreet\",\n",
    "        \"Adresse_Haus-Nr._Standort\": \"SiteHouseNumber\", \"Adresse_Postleitzahl_Standort\": \"SitePostalCode\",\n",
    "        \"Adresse_Ort_Standort\": \"SiteCity\", \"Telefonvorwahl/-nummer\": \"PhoneNumber\",\n",
    "        \"E-Mail Adresse\": \"EmailAddress\", \"Internet-Adresse\": \"InternetAddress\",\n",
    "        \"Traeger\": \"OwnerType\", \"T_Name\": \"OwnerName\", \"EinrichtungsTyp\": \"InstitutionType\",\n",
    "        \"Allgemeine_Notfallversorgung\": \"GeneralEmergencyCare\", \"INSG\": \"TotalBeds\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            header = [h.strip() for h in next(reader)] # Read and clean header\n",
    "\n",
    "            # Adjust header to handle repeated \"Spezielle_Notfallversorgung\" columns\n",
    "            processed_header = []\n",
    "            special_count = 0\n",
    "            for h in header:\n",
    "                if h == \"Spezielle_Notfallversorgung\":\n",
    "                    # Use the SQL column name directly for later matching\n",
    "                    processed_header.append(f\"SpecialEmergencyCare_Modul{special_count + 1}\")\n",
    "                    special_count += 1\n",
    "                else:\n",
    "                    processed_header.append(h)\n",
    "\n",
    "            for row in reader:\n",
    "                if not row: continue # Skip empty rows\n",
    "\n",
    "                row_data = {}\n",
    "                for i, cell in enumerate(row):\n",
    "                    if i >= len(processed_header):\n",
    "                        # print(f\"Warning: More columns in row than in header. Row: {row}\")\n",
    "                        break # Prevent IndexError\n",
    "\n",
    "                    csv_col_name = processed_header[i]\n",
    "                    \n",
    "                    # Try to map directly if it's one of the fixed columns\n",
    "                    sql_col_name = column_mapping.get(csv_col_name, None)\n",
    "\n",
    "                    if sql_col_name:\n",
    "                        # Convert data types for known columns\n",
    "                        if sql_col_name in [\"OwnerType\", \"InstitutionType\", \"GeneralEmergencyCare\", \"TotalBeds\"]:\n",
    "                            try:\n",
    "                                row_data[sql_col_name] = int(cell.strip()) if cell.strip() else None\n",
    "                            except ValueError:\n",
    "                                row_data[sql_col_name] = None\n",
    "                        else:\n",
    "                            row_data[sql_col_name] = cell.strip() if cell.strip() else None\n",
    "                    elif csv_col_name.startswith(\"SpecialEmergencyCare_Modul\"):\n",
    "                        row_data[csv_col_name] = cell.strip() if cell.strip() else None\n",
    "                    else:\n",
    "                        # Handle bed columns\n",
    "                        # Remove special characters and replace dot with underscore for SQL column name\n",
    "                        # E.g.: \"100.0 - 118.0\" -> \"Beds_100_118\"\n",
    "                        bed_col_name_raw = csv_col_name.replace('.', '').replace('-', '_').replace(' ', '')\n",
    "                        sql_bed_col_name = f\"Beds_{bed_col_name_raw}\"\n",
    "                        try:\n",
    "                            row_data[sql_bed_col_name] = int(cell.strip()) if cell.strip() else None\n",
    "                        except ValueError:\n",
    "                            row_data[sql_bed_col_name] = None\n",
    "                data.append(row_data)\n",
    "        print(f\"Read {len(data)} lines from comprehensive hospital facilities file: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading comprehensive hospital facilities file {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def insert_hospital_comprehensive_data(config, data_rows):\n",
    "    \"\"\"\n",
    "    Inserts data into the HospitalFacilitiesComprehensive table.\n",
    "    \"\"\"\n",
    "    if not data_rows:\n",
    "        print(\"No comprehensive hospital facilities data to insert.\")\n",
    "        return\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Get the list of all SQL table columns to ensure correct order\n",
    "        cursor.execute(\"DESCRIBE HospitalFacilitiesComprehensive\")\n",
    "        table_columns_info = cursor.fetchall()\n",
    "        sql_columns = [col[0] for col in table_columns_info if col[0] != 'id'] # Exclude 'id'\n",
    "\n",
    "        # Prepare data for executemany\n",
    "        values_to_insert = []\n",
    "        for row_data in data_rows:\n",
    "            row_values = []\n",
    "            for col in sql_columns:\n",
    "                value = row_data.get(col, None)\n",
    "                if value is None and col.startswith(\"Beds_\"):\n",
    "                    # Try to find the column name in the original CSV format if SQL name not found\n",
    "                    # E.g.: Beds_100_118 (SQL) -> 100.0 - 118.0 (CSV)\n",
    "                    # E.g.: Beds_2953 (SQL) -> 2953 (CSV)\n",
    "                    original_csv_format = col.replace(\"Beds_\", \"\")\n",
    "                    if '_' in original_csv_format: # If it has underscore, it might be a range or decimal\n",
    "                        if original_csv_format.count('_') == 1 and original_csv_format.replace('_', '').isdigit():\n",
    "                             # Case like 100_0 -> 100.0\n",
    "                            original_csv_format = original_csv_format.replace('_', '.')\n",
    "                        elif original_csv_format.count('_') == 2 and original_csv_format.split('_')[0].isdigit() and original_csv_format.split('_')[1].isdigit() and original_csv_format.split('_')[2].isdigit():\n",
    "                            # Case like 1600_59 -> 1600.0 - 59.0\n",
    "                            parts = original_csv_format.split('_')\n",
    "                            original_csv_format = f\"{parts[0]}.0 - {parts[1]}.0\" # Assuming \"X.0 - Y.0\" format\n",
    "                            # Correction for specific \"X.0 - Y.0\" formats\n",
    "                            if original_csv_format == \"1600.0 - 59.0\":\n",
    "                                original_csv_format = \"1600.0 - 59.0\" # Keep as is\n",
    "                            elif original_csv_format == \"1700.0 - 35.0\":\n",
    "                                original_csv_format = \"1700.0 - 35.0\"\n",
    "                            elif original_csv_format == \"1800.0 - 28.0\":\n",
    "                                original_csv_format = \"1800.0 - 28.0\"\n",
    "                            elif original_csv_format == \"2200.0 - 39.0\":\n",
    "                                original_csv_format = \"2200.0 - 39.0\"\n",
    "                            elif original_csv_format == \"2400.0 - 41.0\":\n",
    "                                original_csv_format = \"2400.0 - 41.0\"\n",
    "                            elif original_csv_format == \"2800.0 - 46.0\":\n",
    "                                original_csv_format = \"2800.0 - 46.0\"\n",
    "                            elif original_csv_format == \"1000.0 - 34.0\":\n",
    "                                original_csv_format = \"1000.0 - 34.0\"\n",
    "                            elif original_csv_format == \"2300.0 - 59.0\": # Hypothetical example\n",
    "                                original_csv_format = \"2300.0 - 59.0\"\n",
    "                            elif original_csv_format == \"8500.0 - 86.0\":\n",
    "                                original_csv_format = \"8500.0 - 86.0\"\n",
    "                        else:\n",
    "                            original_csv_format = original_csv_format.replace('_', '.') # Other cases with underscore\n",
    "                    \n",
    "                    value = row_data.get(original_csv_format, None)\n",
    "                row_values.append(value)\n",
    "            values_to_insert.append(tuple(row_values))\n",
    "\n",
    "        placeholders = ', '.join(['%s'] * len(sql_columns))\n",
    "        insert_statement = f\"INSERT INTO HospitalFacilitiesComprehensive ({', '.join(sql_columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "        print(f\"\\nInserting {len(values_to_insert)} rows into HospitalFacilitiesComprehensive...\")\n",
    "        cursor.executemany(insert_statement, values_to_insert)\n",
    "        connection.commit()\n",
    "        print(f\"{cursor.rowcount} rows inserted successfully.\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting comprehensive hospital facilities data: {e}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "def read_hospital_summarized_data_from_csv(file_path):\n",
    "    \"\"\"Reads summarized hospital facilities data from a CSV file.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Mapping CSV column names to SQL column names\n",
    "    column_mapping = {\n",
    "        \"Land\": \"Land\", \"Kreis\": \"Kreis\", \"Gemeinde\": \"Gemeinde\", \"Adresse_Name\": \"HospitalName\",\n",
    "        \"Adresse_Straße\": \"Street\", \"Adresse_Haus-Nr.\": \"HouseNumber\",\n",
    "        \"Adresse_Postleitzahl\": \"PostalCode\", \"Adresse_Ort\": \"City\",\n",
    "        \"Telefonvorwahl/-nummer\": \"PhoneNumber\", \"E-Mail Adresse\": \"EmailAddress\",\n",
    "        \"Internet-Adresse\": \"InternetAddress\", \"Traeger\": \"OwnerType\", \"T-Name\": \"OwnerName\",\n",
    "        \"EinrichtungsTyp\": \"InstitutionType\", \"INSG\": \"TotalBeds\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            header = [h.strip() for h in next(reader)] # Read and clean header\n",
    "\n",
    "            for row in reader:\n",
    "                if not row: continue # Skip empty rows\n",
    "\n",
    "                row_data = {}\n",
    "                for i, cell in enumerate(row):\n",
    "                    if i >= len(header):\n",
    "                        # print(f\"Warning: More columns in row than in header. Row: {row}\")\n",
    "                        break # Prevent IndexError\n",
    "\n",
    "                    csv_col_name = header[i]\n",
    "                    sql_col_name = column_mapping.get(csv_col_name, None)\n",
    "\n",
    "                    if sql_col_name:\n",
    "                        # Convert data types for known columns\n",
    "                        if sql_col_name in [\"OwnerType\", \"InstitutionType\", \"TotalBeds\"]:\n",
    "                            try:\n",
    "                                row_data[sql_col_name] = int(cell.strip()) if cell.strip() else None\n",
    "                            except ValueError:\n",
    "                                row_data[sql_col_name] = None\n",
    "                        else:\n",
    "                            row_data[sql_col_name] = cell.strip() if cell.strip() else None\n",
    "                    else:\n",
    "                        # Handle bed columns\n",
    "                        # Remove special characters and replace dot with underscore for SQL column name\n",
    "                        bed_col_name_raw = csv_col_name.replace('.', '').replace('-', '_').replace(' ', '')\n",
    "                        sql_bed_col_name = f\"Beds_{bed_col_name_raw}\"\n",
    "                        try:\n",
    "                            row_data[sql_bed_col_name] = int(cell.strip()) if cell.strip() else None\n",
    "                        except ValueError:\n",
    "                            row_data[sql_bed_col_name] = None\n",
    "                data.append(row_data)\n",
    "        print(f\"Read {len(data)} lines from summarized hospital facilities file: {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading summarized hospital facilities file {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def insert_hospital_summarized_data(config, data_rows):\n",
    "    \"\"\"\n",
    "    Inserts data into the HospitalFacilitiesSummarized table.\n",
    "    \"\"\"\n",
    "    if not data_rows:\n",
    "        print(\"No summarized hospital facilities data to insert.\")\n",
    "        return\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Get the list of all SQL table columns to ensure correct order\n",
    "        cursor.execute(\"DESCRIBE HospitalFacilitiesSummarized\")\n",
    "        table_columns_info = cursor.fetchall()\n",
    "        sql_columns = [col[0] for col in table_columns_info if col[0] != 'id'] # Exclude 'id'\n",
    "\n",
    "        # Prepare data for executemany\n",
    "        values_to_insert = []\n",
    "        for row_data in data_rows:\n",
    "            row_values = []\n",
    "            for col in sql_columns:\n",
    "                value = row_data.get(col, None)\n",
    "                if value is None and col.startswith(\"Beds_\"):\n",
    "                    # Try to find the column name in the original CSV format if SQL name not found\n",
    "                    original_csv_format = col.replace(\"Beds_\", \"\")\n",
    "                    if '_' in original_csv_format: # If it has underscore, it might be a range or decimal\n",
    "                        if original_csv_format.count('_') == 1 and original_csv_format.replace('_', '').isdigit():\n",
    "                            original_csv_format = original_csv_format.replace('_', '.')\n",
    "                        elif original_csv_format.count('_') == 2 and original_csv_format.split('_')[0].isdigit() and original_csv_format.split('_')[1].isdigit() and original_csv_format.split('_')[2].isdigit():\n",
    "                            parts = original_csv_format.split('_')\n",
    "                            original_csv_format = f\"{parts[0]}.0 - {parts[1]}.0\" # Assuming \"X.0 - Y.0\" format\n",
    "                            # Handle the specific \"8500.0 - 86.0\" case\n",
    "                            if original_csv_format == \"8500.0 - 86.0\":\n",
    "                                original_csv_format = \"8500.0 - 86.0\"\n",
    "                        else:\n",
    "                            original_csv_format = original_csv_format.replace('_', '.') # Other cases with underscore\n",
    "                    \n",
    "                    value = row_data.get(original_csv_format, None)\n",
    "                row_values.append(value)\n",
    "            values_to_insert.append(tuple(row_values))\n",
    "\n",
    "        placeholders = ', '.join(['%s'] * len(sql_columns))\n",
    "        insert_statement = f\"INSERT INTO HospitalFacilitiesSummarized ({', '.join(sql_columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "        print(f\"\\nInserting {len(values_to_insert)} rows into HospitalFacilitiesSummarized...\")\n",
    "        cursor.executemany(insert_statement, values_to_insert)\n",
    "        connection.commit()\n",
    "        print(f\"{cursor.rowcount} rows inserted successfully.\")\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error inserting summarized hospital facilities data: {e}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"MySQL connection closed.\")\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Ensure the database exists\n",
    "    if create_database_if_not_exists(DB_CONFIG):\n",
    "        # Step 2: Create the tables within that database\n",
    "        if create_tables(DB_CONFIG, SQL_CREATE_TABLES):\n",
    "            print(\"\\nDatabase and tables setup complete.\")\n",
    "            \n",
    "            # Step 3: Read and insert data from each CSV file\n",
    "            \n",
    "            # Population Projection Data (File 1)\n",
    "            for file_path in CSV_FILE_PATHS['population']:\n",
    "                population_data = read_population_data_from_csv(file_path)\n",
    "                insert_population_data(DB_CONFIG, population_data)\n",
    "\n",
    "            # Hospital Bed Details (File 2)\n",
    "            for file_path in CSV_FILE_PATHS['bed_details']:\n",
    "                bed_details_data = read_hospital_bed_details_from_csv(file_path)\n",
    "                insert_hospital_bed_details_data(DB_CONFIG, bed_details_data)\n",
    "\n",
    "            # Comprehensive Hospital Facilities Data (File 3)\n",
    "            for file_path in CSV_FILE_PATHS['comprehensive']:\n",
    "                comprehensive_data = read_hospital_comprehensive_data_from_csv(file_path)\n",
    "                insert_hospital_comprehensive_data(DB_CONFIG, comprehensive_data)\n",
    "\n",
    "            # Summarized Hospital Facilities Data (File 4)\n",
    "            for file_path in CSV_FILE_PATHS['summarized']:\n",
    "                summarized_data = read_hospital_summarized_data_from_csv(file_path)\n",
    "                insert_hospital_summarized_data(DB_CONFIG, summarized_data)\n",
    "\n",
    "            print(\"\\nCSV data import process completed.\")\n",
    "        else:\n",
    "            print(\"\\nTable creation failed.\")\n",
    "    else:\n",
    "        print(\"\\nDatabase creation failed, aborting table setup.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f1434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d828a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329efdc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_65768\\4129310309.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Connect to the database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m conn = pyodbc.connect(\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;34mr'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdb_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mInterfaceError\u001b[0m: ('IM002', '[IM002] [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified (0) (SQLDriverConnect)')"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Replace with the path to your .accdb file\n",
    "db_path = r\"C:\\Users\\user\\Desktop\\A_AI_DDSP\\Dataset\\raw\\DRG-Statistik (Patientenvolumen)\\2021\\Versand_19_DRG_PatKreis_2021.accdb\"\n",
    "\n",
    "# import pyodbc\n",
    "import csv\n",
    "\n",
    "# Path to your Access database file\n",
    "# db_path = r\"C:\\path\\to\\Versand_19_DRG_PatKreis_2021.accdb\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = pyodbc.connect(\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=' + db_path\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get the list of tables\n",
    "cursor.execute(\"SELECT name FROM MSysObjects WHERE type=1 AND name NOT LIKE 'MSys%'\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "# Loop through tables and extract data\n",
    "for table in tables:\n",
    "    print(f\"Extracting data from table: {table}\")\n",
    "    \n",
    "    # Fetch all data\n",
    "    cursor.execute(f\"SELECT * FROM {table}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Save data to CSV\n",
    "    with open(f\"{table}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([column[0] for column in cursor.description])  # Write headers\n",
    "        writer.writerows(rows)  # Write data\n",
    "\n",
    "    print(f\"Data from {table} exported successfully to {table}.csv\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151fb0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL Server', 'ODBC Driver 18 for SQL Server']\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e472cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
